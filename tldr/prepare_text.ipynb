{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train and test sets\n",
    "\n",
    "For the summary generating model, I decided to focus on only the articles associated with crime news (cluster 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251328"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load articles\n",
    "clustered_articles_dict = pickle.load(open(\"clustered_articles_dict.pkl\", \"rb\"))\n",
    "len(clustered_articles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27760\n"
     ]
    }
   ],
   "source": [
    "# extract articles from cluster 0\n",
    "# cluster 0 is crime-related articles\n",
    "crime_dict = [article_dict for article_dict in clustered_articles_dict if article_dict[\"cluster\"]==0]\n",
    "print(len(crime_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223568\n"
     ]
    }
   ],
   "source": [
    "# extract articles from other clusters\n",
    "other_topic_dict = [article_dict for article_dict in clustered_articles_dict if article_dict[\"cluster\"]!=0 and article_dict[\"cluster\"]]\n",
    "random.seed(918)\n",
    "random.shuffle(other_topic_dict)\n",
    "print(len(other_topic_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train articles: 27260\n",
      "Number of test articles: 500\n"
     ]
    }
   ],
   "source": [
    "# create train and test set from the crime-related articles\n",
    "# set aside 500 articles for testing\n",
    "num_training = 500\n",
    "\n",
    "random.seed(910)\n",
    "random.shuffle(crime_dict)\n",
    "test_articles = crime_dict[:num_training]\n",
    "train_articles = crime_dict[num_training:]\n",
    "\n",
    "print(\"Number of train articles: {}\".format(len(train_articles)))\n",
    "print(\"Number of test articles: {}\".format(len(test_articles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle for future use\n",
    "pickle.dump(train_articles, open(\"train_articles_dict.pkl\", \"wb\"))\n",
    "pickle.dump(test_articles, open(\"test_articles_dict.pkl\", \"wb\"))\n",
    "pickle.dump(other_topic_dict[:500], open(\"other_topic_test_dict.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare articles and highlights for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_articles_dict = pickle.load(open(\"train_articles_dict.pkl\", \"rb\"))\n",
    "test_articles_dict = pickle.load(open(\"test_articles_dict.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train articles: 27260\n",
      "Number of test articles: 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train articles: {}\".format(len(train_articles_dict)))\n",
    "print(\"Number of test articles: {}\".format(len(test_articles_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article': 'CNN reason believe little girl spotted India Madeleine McCann British girl missing years family spokesman said Thursday Reports spotting girl bearing resemblance child disappeared vacation parents Portugal sparked frenzy Twitter Clarence Mitchell spokesman parents Kate Gerry said latest reports credible tips proved incorrect years learnt reports seriously Mitchell said parents adding suggest breakthrough said aware requests DNA match girl seen Leh northern India regional police chief India said knew sighting report originated Indian newspaper recovered girl Leh said Abdul Gani Mir deputy inspector general police central Kashmir Leh question carrying DNA girl checked checked officers ground Leh categorically told recovered girl Mir told CNN Madeleine McCann years old disappeared condo resort Portugal parents dined restaurant nearby Journalist Mukhtar Ahmad Srinagar contributed report',\n",
       "  'cluster': 0,\n",
       "  'file': './cnn/stories/99f5e941b36b56c273fc9582c6526d310acd16e4.story',\n",
       "  'highlights': 'Reports of a sighting of the missing girl spark an online frenzy A spokesman for her family says there is no reason to think this is a breakthrough Police in India say they do not have a girl to test'},\n",
       " {'article': 'Red faced police chiefs forced stolen vehicle alert cars taken driveway officer ’s house Metropolitan Police officer parked unmarked blue Skoda Superb estate outside home Brentwood Essex Thursday night thief broke house snatched car keys 25000 vehicle fitted discreet blue lights tone horns Scotland Yard spokesman said want motorists aware car driven police officer pull requested driver driver likely individual stole it’ Skoda Superb stolen driveway outside police officers house Brentwood Essex file photo Members public warned pulling challenged stolen police car fitted siren flashing lights',\n",
       "  'cluster': 0,\n",
       "  'file': './dailymail/stories/190b56a6505f6f8f40fcda4a3e90400699ca6000.story',\n",
       "  'highlights': 'The unmarked police car was parked outside the officers Essex home The thief broke into the house and took the cars keys before escaping The Skoda Superb was equipped with a siren and discreet blue lights'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of the articles and summaries\n",
    "train_articles_dict[100:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27260\n",
      "27260\n"
     ]
    }
   ],
   "source": [
    "# extract articles and summaries\n",
    "train_articles = [article[\"article\"] for article in train_articles_dict]\n",
    "train_highlights = [article[\"highlights\"] for article in train_articles_dict]\n",
    "\n",
    "print(len(train_articles))\n",
    "print(len(train_highlights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# last minute cleaning\n",
    "train_articles = [re.sub(r\"\\'\", \"\", article) for article in train_articles]\n",
    "train_highlights = [re.sub(r\"\\'\", \"\", article) for article in train_highlights]\n",
    "\n",
    "train_articles = [re.sub(r\"million[s]*\", \" million\", article) for article in train_articles]\n",
    "train_highlights = [re.sub(r\"million[s]*\", \" million\", article) for article in train_highlights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    \"\"\"\n",
    "    Create a dictionary with the number of occurrences of each word.\n",
    "    \"\"\"\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 111409\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary from articles and highlights (with number of times each word was used)\n",
    "word_counts = {}\n",
    "\n",
    "count_words(word_counts, train_articles)\n",
    "count_words(word_counts, train_highlights)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing words in the vocabulary dictionary with the words in different pre-trained word embeddings, Word2Vec Google News accounted for the most words in the vocabulary dictionary.\n",
    "\n",
    "The Word2Vec Google News was trained on part of Google News dataset, and contains 300-dimensional vectors for 3 million words and phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and unzip Word2Vec Google News\n",
    "!curl -O https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
    "!gunzip GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word vectors in word2vec: 3000000\n",
      "Length of embedding: 300\n"
     ]
    }
   ],
   "source": [
    "# load and examine Word2Vec Google News\n",
    "word2vec = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "print(\"Number of word vectors in word2vec: {}\".format(len(word2vec.vocab)))\n",
    "print(\"Length of embedding: {}\".format(len(word2vec.word_vec(\"jump\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 3000000\n",
      "Embedding length: 300\n"
     ]
    }
   ],
   "source": [
    "# create dictionary of \n",
    "embeddings_index = {}\n",
    "for word in list(word2vec.vocab):\n",
    "    embeddings_index[word] = word2vec.word_vec(word)\n",
    "        \n",
    "print('Word embeddings:', len(embeddings_index))\n",
    "print('Embedding length:', len(word2vec.word_vec(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 614\n",
      "Percent of words that are missing from vocabulary: 0.5499999999999999%\n"
     ]
    }
   ],
   "source": [
    "# count the number of missing words (that are used more than 20 times threshold)\n",
    "num_missing_words = 0\n",
    "threshold = 20\n",
    "missing_words = []\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in word2vec.vocab:\n",
    "            missing_words.append(word)\n",
    "            num_missing_words += 1\n",
    "            \n",
    "missing_ratio = (num_missing_words/len(word_counts))*100\n",
    "            \n",
    "print(\"Number of words missing from CN: {}\".format(num_missing_words))\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for converting words to an index value\n",
    "vocab_to_int = {} \n",
    "\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in word2vec.vocab:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information about the special symbols, check out: https://medium.com/towards-data-science/sequence-to-sequence-model-introduction-and-concepts-44d9b41cd42d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add special symbols to existing vocab_to_int dictionary\n",
    "codes = [\"<PAD>\",\"<UNK>\",\"<GO>\"]   \n",
    "\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dictionary for convert index value to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 111409\n",
      "Number of words included: 87889\n",
      "Percent of words included: 78.8885996643%\n"
     ]
    }
   ],
   "source": [
    "usage_ratio = (len(vocab_to_int) / len(word_counts))*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words included:\", len(vocab_to_int))\n",
    "print(\"Percent of words included: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle for future use\n",
    "pickle.dump(vocab_to_int, open(\"./model_files/vocab_to_int.pkl\", \"wb\"))\n",
    "pickle.dump(int_to_vocab, open(\"./model_files/int_to_vocab.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87889\n"
     ]
    }
   ],
   "source": [
    "# create matrix of word embeddings for each word in vocab_to_int dictionary\n",
    "embedding_dim = 300 # length of word embedding vectors\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in word2vec.vocab:\n",
    "        word_embedding_matrix[i] = word2vec.word_vec(word)\n",
    "    else:\n",
    "        # create a vector of random numbers if the word is not included in the Word2Vec\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "print(len(word_embedding_matrix))\n",
    "\n",
    "# pickle for future use\n",
    "pickle.dump(word_embedding_matrix, open(\"./model_files/word_embedding_matrix.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_ints(list_text, word_count, unk_count):\n",
    "    \"\"\"\n",
    "    Convert each work in text into an integer, while counting the total number of words and <UNK>. Include <EOS> at the end of the text.\n",
    "    \"\"\"\n",
    "    ints = []\n",
    "    word_count = 0\n",
    "    unk_count = 0\n",
    "    \n",
    "    for sentence in list_text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "        \n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 5371728\n",
      "Total number of UNKs in headlines: 62941\n",
      "Percent of words that are UNK: 1.17%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "int_summaries, word_count_summaries, unk_count_summaries = convert_to_ints(train_highlights)\n",
    "int_articles, word_count_articles, unk_count_articles = convert_to_ints(train_articles)\n",
    "\n",
    "total_word_count = word_count_summaries + word_count_articles\n",
    "total_unk_count = unk_count_summaries + unk_count_summaries\n",
    "\n",
    "print(\"Total number of words in headlines: {}\".format(total_word_count))\n",
    "print(\"Total number of UNKs in headlines: {}\".format(total_unk_count))\n",
    "print(\"Percent of words that are UNK: {}%\".format(total_unk_count/total_word_count,4)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unk_counter(int_text):\n",
    "    \"\"\"\n",
    "    Count the total number of <UNK> in text.\n",
    "    int_text is vector of integers representing text (string)\n",
    "    \"\"\"\n",
    "    unk_count = 0\n",
    "    for word in int_text:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12653\n",
      "12653\n"
     ]
    }
   ],
   "source": [
    "# articles with more than 20 <UNK> are excluded\n",
    "unk_limit = 20\n",
    "sorted_summaries = []\n",
    "sorted_articles = []\n",
    "\n",
    "# get length of each article\n",
    "len_articles = [len(int_articles) for article in int_articles]\n",
    "\n",
    "# sort the summaries and articles by the length of the articles from shortest to longest to reduce the number of <PAD> added\n",
    "for length in range(min(len_articles), max(len_articles)+1):\n",
    "    for index, words in enumerate(int_summaries):\n",
    "        if (unk_counter(int_summaries[index]) <= unk_limit and\n",
    "            unk_counter(int_articles[index]) <= unk_limit and\n",
    "            length == len(int_articles[index])):\n",
    "            sorted_summaries.append(int_summaries[index])\n",
    "            sorted_articles.append(int_articles[index])\n",
    "        \n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_articles))\n",
    "\n",
    "# pickle for future use\n",
    "pickle.dump(sorted_summaries, open(\"./model_files/sorted_summaries.pkl\",\"wb\"))\n",
    "pickle.dump(sorted_articles, open(\"./model_files/sorted_articles.pkl\",\"wb\"))\n",
    "pickle.dump(len(sorted_articles), open(\"./model_files/sorted_articles_length.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_text_batch(text_batch):\n",
    "    \"\"\"\n",
    "    Add <PAD> so that each text within the same batch has the same length.\n",
    "    \"\"\"\n",
    "    max_text = max([len(text) for text in text_batch])\n",
    "    return [text + [vocab_to_int[\"<PAD>\"]] * (max_text - len(text)) for text in text_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches created: 197\n"
     ]
    }
   ],
   "source": [
    "def get_batches(summaries, articles, batch_size):\n",
    "    \"\"\"\n",
    "    Create dictionaries of individual batches with articles, summaries, and the respective lengths. \n",
    "    This way all of the articles and aummaries don't have to be loaded into memory at the same time.\n",
    "    \"\"\"\n",
    "    # create directory named \"batches\"\n",
    "    !mkdir batches\n",
    "    \n",
    "    num_batches = len(articles)//batch_size\n",
    "\n",
    "    # create and pickle a dict of relevant info (articles and summaries) for each batch\n",
    "    for batch_i in range(0, num_batches):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        articles_batch = articles[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_text_batch(summaries_batch))\n",
    "        pad_articles_batch = np.array(pad_text_batch(articles_batch))\n",
    "\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "\n",
    "        pad_articles_lengths = []\n",
    "        for article in pad_articles_batch:\n",
    "            pad_articles_lengths.append(len(article))\n",
    "\n",
    "        data = {\"summaries_batch\":pad_summaries_batch, \"articles_batch\":pad_articles_batch, \n",
    "                \"summaries_lengths\":pad_summaries_lengths, \"articles_lengths\":pad_articles_lengths}\n",
    "        file = \"./batches/batch{}.pkl\".format(batch_i)\n",
    "        pickle.dump(data, open(file,\"wb\"))\n",
    "    \n",
    "    print(\"Number of batches created: {}\".format(num_batches))\n",
    "        \n",
    "get_batches(sorted_summaries, sorted_articles, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
